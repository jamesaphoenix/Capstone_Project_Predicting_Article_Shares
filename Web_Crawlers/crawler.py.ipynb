{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL: https://docs.python.org/2/library/queue.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exception calling callback for <Future at 0x110dd82b0 state=finished raised NameError>\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.7/concurrent/futures/_base.py\", line 324, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"<ipython-input-2-21ca3fec0eab>\", line 34, in post_scrape_callback\n",
      "    result = res.result()\n",
      "  File \"/anaconda3/lib/python3.7/concurrent/futures/_base.py\", line 425, in result\n",
      "    return self.__get_result()\n",
      "  File \"/anaconda3/lib/python3.7/concurrent/futures/_base.py\", line 384, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/anaconda3/lib/python3.7/concurrent/futures/thread.py\", line 57, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"<ipython-input-2-21ca3fec0eab>\", line 42, in scrape_page\n",
      "    self.article = Article(url)\n",
      "NameError: name 'Article' is not defined\n"
     ]
    }
   ],
   "source": [
    "## Importing Libraries For Multi-threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Queue, Empty\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "class MultiThreadScraper:\n",
    " \n",
    "    def __init__(self, links):\n",
    "        self.pool = ThreadPoolExecutor(max_workers=20)\n",
    "        self.scraped_pages = set([])\n",
    "        self.to_crawl = Queue()\n",
    "        self.start_url = 'https://docs.python.org/2/library/queue.html'\n",
    "        self.to_crawl.put(self.start_url)\n",
    "        self.links = ['https://stackoverflow.com/questions/44345139/python-asyncio-add-done-callback-with-async-def',\n",
    "                     'https://edmundmartin.com/using-asyncio-some-examples-and-patterns/']\n",
    "       \n",
    "        self.master_dict = {\n",
    "            'Titles':[],\n",
    "            'Test': []\n",
    "        }\n",
    "       \n",
    " \n",
    "    def parse_links(self, html):\n",
    "        for url in self.links:\n",
    "            if url not in self.scraped_pages:\n",
    "                self.to_crawl.put(url)\n",
    " \n",
    "    def scrape_info(self, html):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        self.master_dict['Titles'].append(soup.title)\n",
    "        self.master_dict['Test'].append('This is a test')\n",
    "        print(self.master_dict)\n",
    " \n",
    "    def post_scrape_callback(self, res):\n",
    "        result = res.result()\n",
    "        if result and result.status_code == 200:\n",
    "            self.parse_links(result.text)\n",
    "            self.scrape_info(result.text)\n",
    " \n",
    "    def scrape_page(self, url):\n",
    "        try:\n",
    "            res = requests.get(url, timeout=(3, 30))\n",
    "            self.article= \n",
    "            return res\n",
    "        except requests.RequestException:\n",
    "            return\n",
    " \n",
    "    def run_scraper(self):\n",
    "        while True:\n",
    "            try:\n",
    "                target_url = self.to_crawl.get(timeout=60)\n",
    "                if target_url not in self.scraped_pages:\n",
    "                    print(\"Scraping URL: {}\".format(target_url))\n",
    "                    self.scraped_pages.add(target_url)\n",
    "                    job = self.pool.submit(self.scrape_page, target_url)\n",
    "                    job.add_done_callback(self.post_scrape_callback)\n",
    "            except Empty:\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "               \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    s = MultiThreadScraper(\"https://edmundmartin.com\")\n",
    "    s.run_scraper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
