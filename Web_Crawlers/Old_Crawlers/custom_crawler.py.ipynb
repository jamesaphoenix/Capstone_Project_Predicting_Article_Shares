{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Libraries - https://github.com/codelucas/newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Dependencies\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlunsplit, urlparse\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Tokenization Of Sentences\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Readability Scores\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "urls = ['https://www.moneysavingexpert.com/travel/travel-tips/', \n",
    "        'https://www.moneysavingexpert.com/deals/deals-hunter/2018/05/how-to-get-free-cat-treats-and-other-pet-tips/', \n",
    "       'https://www.smartinsights.com/internet-advertising/internet-advertising-analytics/display-advertising-clickthrough-rates/'\n",
    "       ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler():\n",
    "    def __init__(self, *args):\n",
    "        self.urls = urls\n",
    "        self.master_dict = {\n",
    "            'URL':[],\n",
    "            'Title_Text': [],\n",
    "            'Title_Tag_Length': [],\n",
    "            'Meta_Description': [],\n",
    "            'Meta_Description_Length': [],\n",
    "            'Content': [],\n",
    "            'Word_Length': [],\n",
    "            'Number_Of_Links': [],\n",
    "            'Links_To_Text_Ratio': [],\n",
    "            'Number_Of_Images':[],\n",
    "            'H1_text': [],\n",
    "            'H1_length': [],\n",
    "            'H2_text': [],\n",
    "            'H2_length': [],    \n",
    "            'H3_text': [],\n",
    "            'H3_length': [],    \n",
    "            'H4_text': [],\n",
    "            'H4_length': [],\n",
    "            'H5_text': [],\n",
    "            'H5_length': [],\n",
    "            'lexicon_count': [],\n",
    "            'Setences_Text': [],\n",
    "            'Number_Of_Sentences': [],\n",
    "            'Flesch_Reading_Ease_formula': [],\n",
    "            'Flesch_Kincaid_Grade_Level': [],\n",
    "            'FOG_Scale': [],\n",
    "            'SMOG_Index': [],\n",
    "            'ARI_Index': [],\n",
    "            'Page_Size_In_Bytes': [],\n",
    "            'Plain_Text_Size': [],\n",
    "            'PLain_Text_Rate': [],\n",
    "            'Encoding_Type': [],\n",
    "            'Status_Code':[],\n",
    "            'seo_friendly_url_characters_check': [],\n",
    "            'seo_friendly_url_relative_length_check': [],\n",
    "            'SSL/HTTPS': []\n",
    "            \n",
    "            ### Potential User Metrics ###\n",
    "            #1.  User Comments (Text) \n",
    "            #2. Number of User Comments\n",
    "            #3. User Comments Sentiment\n",
    "\n",
    "            ### Page Speed Metrics ###\n",
    "            #1. Meaningful Paint\n",
    "            #2. Time To Interactive\n",
    "\n",
    "            ### Link Metrics ###\n",
    "            #1. DR \n",
    "            #2. PageURL Ranking \n",
    "            #3. Number of Referring Domains To Page\n",
    "            #4. Number of Backlinks To Page\n",
    "    \n",
    "}        \n",
    "        \n",
    "        \n",
    "    def get_metadata(self, soup):\n",
    "        #Get Title Tag Of Page\n",
    "        title_text = soup.title.getText()\n",
    "        #Title Tag Length In Characters\n",
    "        title_tag_length = len(title_text)\n",
    "        # First get the meta description tag\n",
    "        description = soup.find('meta', attrs={'name':'og:description'}) or soup.find('meta', attrs={'property':'description'}) or soup.find('meta', attrs={'name':'description'})\n",
    "        # If description meta tag was found, then get the content attribute and save it to db entry\n",
    "        if description:\n",
    "            meta_description = description.get('content') \n",
    "        return title_text, title_tag_length, meta_description\n",
    "    \n",
    "    \n",
    "    def extract_page_features(self, soup):\n",
    "    # kill all script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()    # rip it out\n",
    "\n",
    "        # get text\n",
    "        text = soup.body.get_text()\n",
    "\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "        #Word Length of The Webpage\n",
    "        word_length = len(text)\n",
    "\n",
    "        #Number of Links\n",
    "        body_links = soup.body.find_all('a')\n",
    "        number_of_links = len(body_links)\n",
    "\n",
    "        #Links To Text Ratio\n",
    "        links_to_text_ratio = len(body_links) / word_length\n",
    "\n",
    "        #Number of Images\n",
    "        Number_of_images = len([imgtag for imgtag in soup.body.find_all('img')])\n",
    "\n",
    "        return text, word_length, body_links, number_of_links, links_to_text_ratio, Number_of_images\n",
    "        \n",
    "    \n",
    "    def crawl(self, *args):\n",
    "        for url in self.urls:\n",
    "            \n",
    "            #Loading BeautifulSoup\n",
    "            req = requests.get(url)\n",
    "            response = req.text\n",
    "            soup = BeautifulSoup(response)\n",
    "            \n",
    "            #Meta_data\n",
    "            meta_data = self.get_metadata(soup)\n",
    "            self.master_dict['Title_Text'].append(meta_data[0])\n",
    "            self.master_dict['Title_Tag_Length'].append(meta_data[1])\n",
    "            self.master_dict['Meta_Description'].append(meta_data[2])\n",
    "            \n",
    "            #Extracting_Text_Features\n",
    "            text_features = self.extract_page_features(soup)\n",
    "            print(text_features)\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Crawler(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Title Tag Of Page\n",
    "title_text = soup.title.getText()\n",
    "\n",
    "#Title Tag Length In Characters\n",
    "title_tag_length = len(title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the meta description tag\n",
    "description = soup.find('meta', attrs={'name':'og:description'}) or soup.find('meta', attrs={'property':'description'}) or soup.find('meta', attrs={'name':'description'})\n",
    "\n",
    "# If description meta tag was found, then get the content attribute and save it to db entry\n",
    "if description:\n",
    "    meta_description = description.get('content') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Visible Text From The Webpage\n",
    "\n",
    "# kill all script and style elements\n",
    "for script in soup([\"script\", \"style\"]):\n",
    "    script.extract()    # rip it out\n",
    "\n",
    "# get text\n",
    "text = soup.body.get_text()\n",
    "\n",
    "# break into lines and remove leading and trailing space on each\n",
    "lines = (line.strip() for line in text.splitlines())\n",
    "# break multi-headlines into a line each\n",
    "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "# drop blank lines\n",
    "text = '\\n'.join(chunk for chunk in chunks if chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Length of The Webpage\n",
    "word_length = len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Links\n",
    "body_links = soup.body.find_all('a')\n",
    "number_of_links = len(body_links)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Links To Text Ratio\n",
    "links_to_text_ratio = len(body_links) / word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Images\n",
    "Number_of_images = len([imgtag for imgtag in soup.body.find_all('img')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create A DataFrame Here!!!\n",
    "\n",
    "df = pd.Series({'Title_Tag': title_text, \n",
    "                   'Title_Tag_Length':title_tag_length, \n",
    "                   'Meta_Description':meta_description,\n",
    "                   'Body_Content_Text':text,\n",
    "                   'Word_Length': word_length,\n",
    "                   '<a>_Links_In_Body': number_of_links,\n",
    "                   'Links_To_Text_Ratio': links_to_text_ratio,\n",
    "                   'Number_Of_Images': Number_of_images,\n",
    "                   'URL': test_url\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title_Tag                           Submit Listing | Eastern Daily Press\n",
       "Title_Tag_Length                                                      36\n",
       "Meta_Description       News, sport, entertainment and community infor...\n",
       "Body_Content_Text      Register\\nLog in | Register\\nJobs24\\nLocalsear...\n",
       "Word_Length                                                         6241\n",
       "<a>_Links_In_Body                                                    353\n",
       "Links_To_Text_Ratio                                            0.0565614\n",
       "Number_Of_Images                                                      18\n",
       "URL                     https://www.edp24.co.uk/going-out/submit-listing\n",
       "dtype: object"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_variables(url):\n",
    "    \n",
    "    if 'blog' in url:\n",
    "        df['Blog_In_URL'] = 1\n",
    "    else: \n",
    "        df['Blog_In_URL'] = 0    \n",
    "            \n",
    "    if 'community' in url:\n",
    "        df['Community_In_URL'] = 1\n",
    "    else: \n",
    "        df['Community_In_URL'] = 0\n",
    "        \n",
    "    if ('directory' in url) or ('listing' in url):\n",
    "        df['Directory'] = 1\n",
    "    else: \n",
    "        df['Directory'] = 0\n",
    "        \n",
    "    if ('links' in url) or ('resources' in url):\n",
    "        df['Links_In_URL'] = 1\n",
    "    else: \n",
    "        df['Links_In_URL'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_variables(df['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Header Code\n",
    "\n",
    "h1_text = soup.h1\n",
    "number_of_h1 = len(soup.h1)\n",
    "\n",
    "h2_text = soup.h2.getText()\n",
    "number_of_h2 = len(soup.h2)\n",
    "\n",
    "h3_text = soup.h3.getText()\n",
    "number_of_h3 = len(soup.h3)\n",
    "\n",
    "h4_text = soup.h4.getText()\n",
    "number_of_h4 = len(soup.h4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['h1_text'] = h1_text\n",
    "df['h2_text'] = h2_text\n",
    "df['h3_text'] = h3_text\n",
    "df['h4_text'] = h4_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\james.DESKTOP-36N7TS\n",
      "[nltk_data]     Q\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.edp24.co.uk/going-out/submit-listing'"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Readability Scores For A Given Textual Input\n",
    "\n",
    "lexicon_count = textstat.lexicon_count(text, removepunct=True)\n",
    "number_of_sentences = len(sentences)\n",
    "Flesch_Reading_Ease_formula = textstat.flesch_reading_ease(text)\n",
    "Flesch_Kincaid_Grade_Level = textstat.flesch_kincaid_grade(text)\n",
    "FOG_Scale = textstat.gunning_fog(text)\n",
    "SMOG_Index = textstat.smog_index(text)\n",
    "ARI_Index = textstat.automated_readability_index(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Page_Size_In_Bytes\n",
    "page_size_in_bytes = len(req.content)\n",
    "\n",
    "#Plain_text_size\n",
    "plain_text_size = len(text)\n",
    "\n",
    "#plain_text_rate --> plaintext rate value (plain_text_size / page_size)\n",
    "plain_text_rate = (plain_text_size / page_size_in_bytes) * 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encoding \n",
    "encoding = req.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Status_Code\n",
    "status_code = req.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seo_friendly_url_characters_check -- checking for symbols in accordance with Google recommendations only uppercase and lowercase Latin characters, digits and dashes are allowed ‘true’ - if the test is passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "seo_list = ascii_letters + ascii_lowercase + ascii_uppercase + digits + extra_chars\n",
    "ascii_letters = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "ascii_lowercase = 'abcdefghijklmnopqrstuvwxyz'\n",
    "ascii_uppercase = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "digits = '0123456789'\n",
    "extra_chars = '.|-'\n",
    "    \n",
    "new_url = []\n",
    "\n",
    "for character in awesome:\n",
    "    if character in seo_list:\n",
    "        new_url.append(character)\n",
    "\n",
    "if len(new_url) == len(awesome):\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seo_friendly_url_relative_length_check - url should not be longer than 120 characters      \n",
    "if len(url) < 120:\n",
    "    return True\n",
    "else: \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check \n",
    "\n",
    "if url.contains('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://\n"
     ]
    }
   ],
   "source": [
    "# check the actual URL for the site (SSL Encrypted)\n",
    "\n",
    "if 's' in req.url:\n",
    "    return True\n",
    "else:\n",
    "    return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
